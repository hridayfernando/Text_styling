{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12317067,"sourceType":"datasetVersion","datasetId":7763718}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:22:52.999032Z","iopub.execute_input":"2025-06-30T14:22:52.999293Z","iopub.status.idle":"2025-06-30T14:22:53.313480Z","execution_failed":"2025-06-30T14:37:10.724Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/prithvi.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/gourav.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/houshik.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/anukriti.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/kenny.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/aditya.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/ishika.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/akshat.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/divyanshu.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/kunal.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/naveen.png\n/kaggle/input/stylized-images/stylized_images/original_images1/aditya.jpeg\n/kaggle/input/stylized-images/stylized_images/original_images1/kenny.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/naveen.NEF\n/kaggle/input/stylized-images/stylized_images/original_images1/gourav.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/kunal.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/akshat.NEF\n/kaggle/input/stylized-images/stylized_images/original_images1/Anukriti.JPG\n/kaggle/input/stylized-images/stylized_images/original_images1/prithvi.JPG\n/kaggle/input/stylized-images/stylized_images/original_images1/ishika.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/divyanshu.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/houshik.JPG\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/prithvi.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/gourav.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/houshik.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/anukriti.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/kenny.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/aditya.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/ishika.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/akshat.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/divyanshu.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/kunal.png\n/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1/naveen.png\n/kaggle/input/stylized-images/stylized_images/original_images1/aditya.jpeg\n/kaggle/input/stylized-images/stylized_images/original_images1/kenny.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/naveen.NEF\n/kaggle/input/stylized-images/stylized_images/original_images1/gourav.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/kunal.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/akshat.NEF\n/kaggle/input/stylized-images/stylized_images/original_images1/Anukriti.JPG\n/kaggle/input/stylized-images/stylized_images/original_images1/prithvi.JPG\n/kaggle/input/stylized-images/stylized_images/original_images1/ishika.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/divyanshu.jpg\n/kaggle/input/stylized-images/stylized_images/original_images1/houshik.JPG\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom pathlib import Path\nfrom PIL import Image\nimport random\nimport os\nimport gc\nimport logging\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.models as models\nfrom typing import List, Dict, Union, Tuple\n\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Enhanced configuration with validation\"\"\"\n    model_id: str = \"runwayml/stable-diffusion-v1-5\"\n    image_size: int = 512\n    batch_size: int = 2\n    effective_batch_size: int = 8\n    learning_rate: float = 1e-5\n    num_epochs: int = 100  # Increased for better training\n    save_every: int = 10\n    mixed_precision: bool = True\n    gradient_clip: float = 1.0\n    lora_rank: int = 128  # Increased for more capacity\n    lora_alpha: float = 128\n    validation_split: float = 0.2\n    accumulation_steps: int = 4\n    use_enhanced_loss: bool = True\n    perceptual_loss_weight: float = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:22:53.314695Z","iopub.execute_input":"2025-06-30T14:22:53.315246Z","iopub.status.idle":"2025-06-30T14:23:35.088632Z","shell.execute_reply.started":"2025-06-30T14:22:53.315221Z","shell.execute_reply":"2025-06-30T14:23:35.088077Z"}},"outputs":[{"name":"stderr","text":"2025-06-30 14:23:16.693447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751293397.142831      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751293397.254861      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nclass LoRALinear(nn.Module):\n    \"\"\"Fixed LoRA implementation with proper dtype handling\"\"\"\n    \n    def __init__(self, original_layer: nn.Linear, rank: int = 64, alpha: float = 64):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank\n        \n        # Freeze original layer\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n        \n        # Initialize with same dtype and device as original layer\n        device = original_layer.weight.device\n        dtype = original_layer.weight.dtype\n        \n        self.lora_A = nn.Parameter(\n            torch.randn(rank, original_layer.in_features, device=device, dtype=dtype) * 0.01\n        )\n        self.lora_B = nn.Parameter(\n            torch.zeros(original_layer.out_features, rank, device=device, dtype=dtype)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        original_out = self.original_layer(x)\n        \n        # Ensure dtype consistency\n        if x.dtype != self.lora_A.dtype:\n            x_lora = x.to(self.lora_A.dtype)\n        else:\n            x_lora = x\n        \n        lora_out = F.linear(F.linear(x_lora, self.lora_A), self.lora_B) * self.scaling\n        \n        # Ensure output dtype matches original\n        if lora_out.dtype != original_out.dtype:\n            lora_out = lora_out.to(original_out.dtype)\n        \n        return original_out + lora_out\n    \n    def get_lora_state_dict(self) -> Dict[str, torch.Tensor]:\n        return {\n            'lora_A': self.lora_A,\n            'lora_B': self.lora_B,\n            'scaling': torch.tensor(self.scaling, dtype=self.lora_A.dtype)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.089350Z","iopub.execute_input":"2025-06-30T14:23:35.089823Z","iopub.status.idle":"2025-06-30T14:23:35.096876Z","shell.execute_reply.started":"2025-06-30T14:23:35.089795Z","shell.execute_reply":"2025-06-30T14:23:35.096143Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class EnhancedStyleLoss(nn.Module):\n    \"\"\"Enhanced loss function combining diffusion loss with perceptual loss\"\"\"\n    \n    def __init__(self, device=\"cuda\", perceptual_weight=0.1):\n        super().__init__()\n        # Load VGG for perceptual loss\n        vgg = models.vgg19(weights='VGG19_Weights.DEFAULT').features.to(device).eval()\n        for param in vgg.parameters():\n            param.requires_grad = False\n        \n        self.vgg = vgg\n        self.vgg_layers = [2, 7, 12, 21]  # relu1_2, relu2_2, relu3_3, relu4_2\n        self.mse_loss = nn.MSELoss()\n        self.perceptual_weight = perceptual_weight\n        \n    def get_vgg_features(self, x):\n        \"\"\"Extract VGG features from multiple layers\"\"\"\n        features = []\n        for i, layer in enumerate(self.vgg):\n            x = layer(x)\n            if i in self.vgg_layers:\n                features.append(x)\n        return features\n    \n    def compute_perceptual_loss(self, pred_image, target_image):\n        \"\"\"Compute perceptual loss using VGG features\"\"\"\n        # Normalize images to VGG expected range [0,1]\n        pred_norm = (pred_image + 1) / 2\n        target_norm = (target_image + 1) / 2\n        \n        pred_features = self.get_vgg_features(pred_norm)\n        target_features = self.get_vgg_features(target_norm)\n        \n        perceptual_loss = 0\n        for pred_feat, target_feat in zip(pred_features, target_features):\n            perceptual_loss += self.mse_loss(pred_feat, target_feat)\n        \n        return perceptual_loss\n    \n    def forward(self, noise_pred, noise_target, pred_image=None, target_image=None):\n        \"\"\"Combined loss function\"\"\"\n        # Standard diffusion loss\n        diffusion_loss = self.mse_loss(noise_pred, noise_target)\n        \n        # Add perceptual loss if images are provided\n        if pred_image is not None and target_image is not None:\n            perceptual_loss = self.compute_perceptual_loss(pred_image, target_image)\n            total_loss = diffusion_loss + self.perceptual_weight * perceptual_loss\n            return total_loss, diffusion_loss, perceptual_loss\n        else:\n            return diffusion_loss, diffusion_loss, 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.098541Z","iopub.execute_input":"2025-06-30T14:23:35.098722Z","iopub.status.idle":"2025-06-30T14:23:35.130368Z","shell.execute_reply.started":"2025-06-30T14:23:35.098708Z","shell.execute_reply":"2025-06-30T14:23:35.129589Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class StyleTransferDataset(Dataset):\n    \"\"\"Robust dataset implementation with validation and error handling\"\"\"\n    \n    def __init__(\n        self,\n        original_dir: str,\n        styled_dir: str,\n        style_name: str,\n        image_size: int = 512,\n        augment: bool = True,\n        validate_pairs: bool = True,\n    ):\n        self.original_dir = Path(original_dir)\n        self.styled_dir = Path(styled_dir)\n        self.image_size = image_size\n        self.style_name = style_name\n        \n        # Validate directories\n        self._validate_directories()\n        \n        # Load and validate image pairs\n        self.image_pairs = self._load_and_validate_pairs(validate_pairs)\n        \n        # Setup transforms\n        self.transform = self._setup_transforms(augment)\n        \n        logger.info(f\"Loaded {len(self.image_pairs)} valid image pairs for style: {self.style_name}\")\n    \n    def _validate_directories(self):\n        \"\"\"Validate input directories exist and contain images\"\"\"\n        if not self.original_dir.exists():\n            raise FileNotFoundError(f\"Original directory not found: {self.original_dir}\")\n        if not self.styled_dir.exists():\n            raise FileNotFoundError(f\"Styled directory not found: {self.styled_dir}\")\n        \n        # Check for image files\n        orig_files = list(self.original_dir.glob('*.[jp][pn]g')) + list(self.original_dir.glob('*.[JP][PN]G'))\n        style_files = list(self.styled_dir.glob('*.[jp][pn]g')) + list(self.styled_dir.glob('*.[JP][PN]G'))\n        \n        if not orig_files:\n            raise ValueError(f\"No image files found in {self.original_dir}\")\n        if not style_files:\n            raise ValueError(f\"No image files found in {self.styled_dir}\")\n    \n    def _load_and_validate_pairs(self, validate: bool) -> List[Dict]:\n        \"\"\"Load and validate image pairs with error handling\"\"\"\n        pairs = []\n        \n        # Get all image files\n        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.JPG', '*.JPEG', '*.PNG', '*.BMP']\n        orig_files = []\n        style_files = []\n        \n        for ext in extensions:\n            orig_files.extend(self.original_dir.glob(ext))\n            style_files.extend(self.styled_dir.glob(ext))\n        \n        # Sort for consistent pairing\n        orig_files.sort()\n        style_files.sort()\n        \n        # Create and validate pairs\n        min_len = min(len(orig_files), len(style_files))\n        \n        for i in range(min_len):\n            orig_path = orig_files[i]\n            style_path = style_files[i]\n            \n            if validate and not self._validate_image_pair(orig_path, style_path):\n                logger.warning(f\"Skipping invalid pair: {orig_path.name} - {style_path.name}\")\n                continue\n            \n            pairs.append({\n                'original_path': orig_path,\n                'styled_path': style_path,\n                'style_name': self.style_name\n            })\n        \n        if not pairs:\n            raise ValueError(\"No valid image pairs found\")\n        \n        return pairs\n    \n    def _validate_image_pair(self, orig_path: Path, style_path: Path) -> bool:\n        \"\"\"Validate that both images can be loaded and have reasonable dimensions\"\"\"\n        try:\n            with Image.open(orig_path) as orig_img:\n                with Image.open(style_path) as style_img:\n                    # Check if images are valid and have reasonable size\n                    if orig_img.size[0] < 64 or orig_img.size[1] < 64:\n                        return False\n                    if style_img.size[0] < 64 or style_img.size[1] < 64:\n                        return False\n                    return True\n        except Exception as e:\n            logger.warning(f\"Error validating pair {orig_path.name}: {e}\")\n            return False\n    \n    def _setup_transforms(self, augment: bool) -> transforms.Compose:\n        \"\"\"Setup enhanced image transforms\"\"\"\n        transform_list = [\n            transforms.Resize((self.image_size, self.image_size), interpolation=transforms.InterpolationMode.LANCZOS),\n        ]\n        \n        if augment:\n            transform_list.extend([\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomRotation(degrees=5),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n                transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n            ])\n        \n        transform_list.extend([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n        return transforms.Compose(transform_list)\n    \n    def __len__(self) -> int:\n        return len(self.image_pairs)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:\n        \"\"\"Get a single data sample with error handling\"\"\"\n        try:\n            pair = self.image_pairs[idx]\n            \n            # Load images\n            with Image.open(pair['original_path']) as orig_img:\n                original_image = orig_img.convert('RGB').copy()\n            \n            with Image.open(pair['styled_path']) as style_img:\n                styled_image = style_img.convert('RGB').copy()\n            \n            # Apply transforms\n            original_tensor = self.transform(original_image)\n            styled_tensor = self.transform(styled_image)\n            \n            return original_tensor, styled_tensor, pair['style_name']\n            \n        except Exception as e:\n            logger.error(f\"Error loading sample {idx}: {e}\")\n            # Return a random valid sample instead\n            return self.__getitem__(random.randint(0, len(self) - 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.131140Z","iopub.execute_input":"2025-06-30T14:23:35.131718Z","iopub.status.idle":"2025-06-30T14:23:35.160802Z","shell.execute_reply.started":"2025-06-30T14:23:35.131699Z","shell.execute_reply":"2025-06-30T14:23:35.160145Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\nfrom transformers import CLIPImageProcessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.161625Z","iopub.execute_input":"2025-06-30T14:23:35.162352Z","iopub.status.idle":"2025-06-30T14:23:35.185519Z","shell.execute_reply.started":"2025-06-30T14:23:35.162328Z","shell.execute_reply":"2025-06-30T14:23:35.185010Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DiffusionStyleTransferPipeline:\n    \"\"\"Production-ready diffusion pipeline with memory management\"\"\"\n    \n    def __init__(\n        self,\n        config: TrainingConfig,\n        device: str = \"cuda\",\n        torch_dtype: torch.dtype = torch.float16,\n    ):\n        self.config = config\n        self.device = device\n        self.torch_dtype = torch_dtype\n        self.lora_layers = {}\n        \n        logger.info(\"Initializing Stable Diffusion components...\")\n        \n        # Load components\n        self._load_components()\n        \n        # Setup enhanced loss if enabled\n        if config.use_enhanced_loss:\n            self.enhanced_loss = EnhancedStyleLoss(device=device, perceptual_weight=config.perceptual_loss_weight)\n        \n        # Setup inference pipeline (lazy loading)\n        self.inference_pipeline = None\n        \n        logger.info(f\"Pipeline initialized successfully\")\n    \n    def _load_components(self):\n        \"\"\"Load SD components with proper memory management\"\"\"\n        try:\n            # Load components\n            self.vae = AutoencoderKL.from_pretrained(\n                self.config.model_id, \n                subfolder=\"vae\", \n                torch_dtype=self.torch_dtype\n            )\n            self.text_encoder = CLIPTextModel.from_pretrained(\n                self.config.model_id, \n                subfolder=\"text_encoder\", \n                torch_dtype=self.torch_dtype\n            )\n            self.tokenizer = CLIPTokenizer.from_pretrained(\n                self.config.model_id, \n                subfolder=\"tokenizer\"\n            )\n            self.unet = UNet2DConditionModel.from_pretrained(\n                self.config.model_id, \n                subfolder=\"unet\", \n                torch_dtype=self.torch_dtype\n            )\n            self.scheduler = DDPMScheduler.from_pretrained(\n                self.config.model_id, \n                subfolder=\"scheduler\"\n            )\n            \n            # Move to device\n            self.vae.to(self.device)\n            self.unet.to(self.device)\n            \n            # Keep text encoder on CPU initially to save memory\n            if self.device == \"cuda\":\n                self.text_encoder.to(\"cpu\")\n            else:\n                self.text_encoder.to(self.device)\n            \n            # Freeze base models\n            self.vae.requires_grad_(False)\n            self.text_encoder.requires_grad_(False)\n            self.unet.requires_grad_(False)\n            \n            # Add LoRA to UNet\n            self._add_lora_to_unet()\n            \n        except Exception as e:\n            logger.error(f\"Error loading components: {e}\")\n            raise\n    \n    def _add_lora_to_unet(self):\n        \"\"\"Add LoRA layers with proper module replacement\"\"\"\n        lora_count = 0\n        \n        for name, module in self.unet.named_modules():\n            if isinstance(module, nn.Linear) and any(target in name for target in ['attn', 'to_q', 'to_k', 'to_v', 'to_out']):\n                # Create LoRA layer\n                lora_layer = LoRALinear(\n                    module, \n                    rank=self.config.lora_rank, \n                    alpha=self.config.lora_alpha\n                )\n                \n                # Ensure proper device and dtype\n                lora_layer.to(device=self.device, dtype=self.torch_dtype)\n                \n                # Replace module\n                parent_name = '.'.join(name.split('.')[:-1])\n                child_name = name.split('.')[-1]\n                \n                if parent_name:\n                    parent_module = dict(self.unet.named_modules())[parent_name]\n                    setattr(parent_module, child_name, lora_layer)\n                else:\n                    setattr(self.unet, child_name, lora_layer)\n                \n                self.lora_layers[name] = lora_layer\n                lora_count += 1\n        \n        logger.info(f\"Added {lora_count} LoRA layers to UNet\")\n    \n    def encode_text(self, prompts: List[str]) -> torch.Tensor:\n        \"\"\"Encode text prompts with memory management\"\"\"\n        # Move text encoder to device temporarily\n        self.text_encoder.to(self.device)\n        \n        try:\n            text_inputs = self.tokenizer(\n                prompts,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            \n            with torch.no_grad():\n                text_embeddings = self.text_encoder(text_inputs.input_ids.to(self.device))[0]\n            \n            return text_embeddings\n            \n        finally:\n            # Move back to CPU to save memory\n            if self.device == \"cuda\":\n                self.text_encoder.to(\"cpu\")\n    \n    def create_enhanced_style_prompts(self, style_names: List[str]) -> List[str]:\n        \"\"\"Generate enhanced style transfer prompts\"\"\"\n        ghibli_templates = [\n            \"studio ghibli style, anime artwork, soft watercolor painting, hand-drawn animation, whimsical, detailed backgrounds\",\n            \"miyazaki style illustration, ghibli anime, pastel colors, dreamy atmosphere, nature-inspired, magical realism\",\n            \"ghibli movie style, traditional animation, soft lighting, organic shapes, peaceful scenery, artistic masterpiece\",\n            \"spirited away style, howl's moving castle aesthetic, anime art, vibrant yet soft colors, detailed character design\"\n        ]\n        \n        general_templates = [\n            \"{} style artwork, high quality, detailed, masterpiece\",\n            \"beautiful {} style painting, artistic, vibrant colors\",\n            \"{} art style, professional illustration, stunning\",\n            \"amazing {} style image, creative, high resolution\",\n        ]\n        \n        prompts = []\n        for style_name in style_names:\n            if \"ghibli\" in style_name.lower():\n                template = random.choice(ghibli_templates)\n                prompts.append(template)\n            else:\n                template = random.choice(general_templates)\n                prompts.append(template.format(style_name))\n        \n        return prompts\n    \n    def training_step(\n        self,\n        original_images: torch.Tensor,\n        styled_images: torch.Tensor,\n        style_prompts: List[str],\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Enhanced training step with optional perceptual loss\"\"\"\n        try:\n            batch_size = original_images.shape[0]\n            \n            # Encode images to latent space\n            with torch.no_grad():\n                styled_latents = self.vae.encode(styled_images).latent_dist.sample()\n                styled_latents *= self.vae.config.scaling_factor\n            \n            # Sample noise and timesteps\n            noise = torch.randn_like(styled_latents)\n            timesteps = torch.randint(\n                0, self.scheduler.config.num_train_timesteps, \n                (batch_size,), device=self.device\n            ).long()\n            \n            # Add noise to styled images\n            noisy_latents = self.scheduler.add_noise(styled_latents, noise, timesteps)\n            \n            # Encode text prompts\n            text_embeddings = self.encode_text(style_prompts)\n            \n            # Predict noise\n            noise_pred = self.unet(\n                noisy_latents,\n                timesteps,\n                encoder_hidden_states=text_embeddings,\n            ).sample\n            \n            # Calculate loss\n            if self.config.use_enhanced_loss and hasattr(self, 'enhanced_loss'):\n                total_loss, diffusion_loss, perceptual_loss = self.enhanced_loss(\n                    noise_pred, noise, styled_images, styled_images\n                )\n                return {\n                    \"loss\": total_loss,\n                    \"diffusion_loss\": diffusion_loss,\n                    \"perceptual_loss\": perceptual_loss,\n                    \"noise_pred\": noise_pred,\n                    \"noise\": noise,\n                }\n            else:\n                loss = F.mse_loss(noise_pred, noise, reduction=\"mean\")\n                return {\n                    \"loss\": loss,\n                    \"diffusion_loss\": loss,\n                    \"perceptual_loss\": 0.0,\n                    \"noise_pred\": noise_pred,\n                    \"noise\": noise,\n                }\n            \n        except torch.cuda.OutOfMemoryError:\n            logger.error(\"CUDA out of memory during training step\")\n            torch.cuda.empty_cache()\n            raise\n        except Exception as e:\n            logger.error(f\"Error in training step: {e}\")\n            raise\n    \n    def setup_inference_pipeline(self):\n        \"\"\"Setup inference pipeline with proper safety checker handling\"\"\"\n        if self.inference_pipeline is None:\n            try:\n                # Move text encoder back to device for inference\n                self.text_encoder.to(self.device)\n                \n                self.inference_pipeline = StableDiffusionImg2ImgPipeline(\n                    vae=self.vae,\n                    text_encoder=self.text_encoder,\n                    tokenizer=self.tokenizer,\n                    unet=self.unet,\n                    scheduler=self.scheduler,\n                    safety_checker=None,\n                    requires_safety_checker=False,  # Fixed safety checker warning\n                    feature_extractor=None,\n                )\n                self.inference_pipeline.to(self.device)\n                logger.info(\"Inference pipeline setup complete\")\n                \n            except Exception as e:\n                logger.error(f\"Error setting up inference pipeline: {e}\")\n                raise\n    \n    def stylize_image(\n        self,\n        image: Union[str, Image.Image],\n        prompt: str,\n        strength: float = 0.8,  # Higher strength for better style transfer\n        guidance_scale: float = 12.0,  # Higher guidance for better prompt adherence\n        num_inference_steps: int = 75,  # More steps for better quality\n        negative_prompt: str = \"blurry, low quality, distorted, ugly, bad anatomy\"\n    ) -> Image.Image:\n        \"\"\"Apply learned style to an image with enhanced parameters\"\"\"\n        try:\n            self.setup_inference_pipeline()\n            \n            if isinstance(image, str):\n                with Image.open(image) as img:\n                    image = img.convert('RGB').copy()\n            \n            # Resize image\n            image = image.resize((self.config.image_size, self.config.image_size), Image.LANCZOS)\n            \n            # Generate styled image\n            with torch.no_grad():\n                result = self.inference_pipeline(\n                    prompt=prompt,\n                    image=image,\n                    strength=strength,\n                    guidance_scale=guidance_scale,\n                    num_inference_steps=num_inference_steps,\n                    negative_prompt=negative_prompt,\n                )\n            \n            return result.images[0]\n            \n        except Exception as e:\n            logger.error(f\"Error during image stylization: {e}\")\n            raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.186127Z","iopub.execute_input":"2025-06-30T14:23:35.186392Z","iopub.status.idle":"2025-06-30T14:23:35.208358Z","shell.execute_reply.started":"2025-06-30T14:23:35.186367Z","shell.execute_reply":"2025-06-30T14:23:35.207651Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class StyleTransferTrainer:\n    \"\"\"Production-ready trainer with all improvements\"\"\"\n    \n    def __init__(self, pipeline: DiffusionStyleTransferPipeline, config: TrainingConfig):\n        self.pipeline = pipeline\n        self.config = config\n        \n        # Setup optimizer for LoRA parameters only\n        self._setup_optimizer()\n        \n        # Setup mixed precision with updated API\n        self.scaler = torch.amp.GradScaler('cuda') if config.mixed_precision and torch.cuda.is_available() else None\n        \n        # Training metrics\n        self.train_losses = []\n        self.val_losses = []\n        \n        logger.info(f\"Trainer initialized with {len(self._get_lora_params())} LoRA parameters\")\n    \n    def _get_lora_params(self) -> List[torch.nn.Parameter]:\n        \"\"\"Get all LoRA parameters\"\"\"\n        lora_params = []\n        for lora_layer in self.pipeline.lora_layers.values():\n            lora_params.extend(lora_layer.parameters())\n        return lora_params\n    \n    def _setup_optimizer(self):\n        \"\"\"Setup optimizer with learning rate scheduling\"\"\"\n        lora_params = self._get_lora_params()\n        \n        self.optimizer = optim.AdamW(\n            lora_params,\n            lr=self.config.learning_rate,\n            weight_decay=0.01,\n            betas=(0.9, 0.999)\n        )\n        \n        # Learning rate scheduler\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer,\n            T_max=self.config.num_epochs,\n            eta_min=self.config.learning_rate * 0.1\n        )\n    \n    def train_epoch(self, dataloader: DataLoader, epoch: int) -> float:\n        \"\"\"Train for one epoch with gradient accumulation and enhanced loss logging\"\"\"\n        total_loss = 0\n        total_diffusion_loss = 0\n        total_perceptual_loss = 0\n        num_batches = len(dataloader)\n        \n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n        \n        self.optimizer.zero_grad()\n        \n        for batch_idx, (original_images, styled_images, batch_style_names) in enumerate(progress_bar):\n            try:\n                # Move to device\n                original_images = original_images.to(self.pipeline.device)\n                styled_images = styled_images.to(self.pipeline.device)\n                \n                # Create enhanced prompts\n                prompts = self.pipeline.create_enhanced_style_prompts(batch_style_names)\n                \n                # Training step with mixed precision (updated API)\n                if self.scaler:\n                    with torch.amp.autocast('cuda'):\n                        outputs = self.pipeline.training_step(\n                            original_images=original_images,\n                            styled_images=styled_images,\n                            style_prompts=prompts,\n                        )\n                        loss = outputs[\"loss\"] / self.config.accumulation_steps\n                    \n                    self.scaler.scale(loss).backward()\n                else:\n                    outputs = self.pipeline.training_step(\n                        original_images=original_images,\n                        styled_images=styled_images,\n                        style_prompts=prompts,\n                    )\n                    loss = outputs[\"loss\"] / self.config.accumulation_steps\n                    loss.backward()\n                \n                total_loss += loss.item() * self.config.accumulation_steps\n                total_diffusion_loss += outputs.get(\"diffusion_loss\", 0)\n                total_perceptual_loss += outputs.get(\"perceptual_loss\", 0)\n                \n                # Gradient accumulation step\n                if (batch_idx + 1) % self.config.accumulation_steps == 0:\n                    if self.scaler:\n                        self.scaler.unscale_(self.optimizer)\n                        torch.nn.utils.clip_grad_norm_(\n                            self._get_lora_params(),\n                            self.config.gradient_clip\n                        )\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                    else:\n                        torch.nn.utils.clip_grad_norm_(\n                            self._get_lora_params(),\n                            self.config.gradient_clip\n                        )\n                        self.optimizer.step()\n                    \n                    self.optimizer.zero_grad()\n                \n                # Update progress bar with detailed loss info\n                progress_bar.set_postfix({\n                    'Total': f'{loss.item() * self.config.accumulation_steps:.4f}',\n                    'Diffusion': f'{outputs.get(\"diffusion_loss\", 0):.4f}',\n                    'Perceptual': f'{outputs.get(\"perceptual_loss\", 0):.4f}',\n                    'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n                })\n                 \n            except torch.cuda.OutOfMemoryError:\n                logger.error(\"CUDA OOM during training - clearing cache\")\n                torch.cuda.empty_cache()\n                self.optimizer.zero_grad()\n                continue\n            except Exception as e:\n                logger.error(f\"Error in batch {batch_idx}: {e}\")\n                continue\n        \n        avg_loss = total_loss / num_batches\n        self.train_losses.append(avg_loss)\n        \n        # Step scheduler\n        self.scheduler.step()\n        \n        # Log detailed loss information\n        logger.info(f\"Epoch {epoch} - Total: {avg_loss:.4f}, \"\n                    f\"Diffusion: {total_diffusion_loss/num_batches:.4f}, \"\n                    f\"Perceptual: {total_perceptual_loss/num_batches:.4f}\")\n        \n        return avg_loss\n    \n    def validate(self, val_dataloader: DataLoader) -> float:\n        \"\"\"Validation loop\"\"\"\n        total_val_loss = 0\n        num_batches = len(val_dataloader)\n        \n        with torch.no_grad():\n            for original_images, styled_images, batch_style_names in val_dataloader:\n                try:\n                    original_images = original_images.to(self.pipeline.device)\n                    styled_images = styled_images.to(self.pipeline.device)\n                    \n                    prompts = self.pipeline.create_enhanced_style_prompts(batch_style_names)\n                    \n                    outputs = self.pipeline.training_step(\n                        original_images=original_images,\n                        styled_images=styled_images,\n                        style_prompts=prompts,\n                    )\n                    \n                    total_val_loss += outputs[\"loss\"].item()\n                    \n                except Exception as e:\n                    logger.warning(f\"Error in validation batch: {e}\")\n                    continue\n        \n        avg_val_loss = total_val_loss / num_batches if num_batches > 0 else float('inf')\n        self.val_losses.append(avg_val_loss)\n        \n        return avg_val_loss\n    \n    def save_model(self, path: str, epoch: int = None):\n        \"\"\"Save LoRA weights and training state\"\"\"\n        try:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            \n            # Collect LoRA state dicts\n            lora_state_dict = {}\n            for name, lora_layer in self.pipeline.lora_layers.items():\n                lora_state_dict[name] = lora_layer.get_lora_state_dict()\n            \n            # Save checkpoint\n            checkpoint = {\n                'lora_state_dict': lora_state_dict,\n                'optimizer_state_dict': self.optimizer.state_dict(),\n                'scheduler_state_dict': self.scheduler.state_dict(),\n                'config': self.config,\n                'epoch': epoch,\n                'train_losses': self.train_losses,\n                'val_losses': self.val_losses,\n            }\n            \n            torch.save(checkpoint, path)\n            logger.info(f\"Model saved to {path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error saving model: {e}\")\n            raise\n    \n    def load_model(self, path: str):\n        \"\"\"Load LoRA weights and training state\"\"\"\n        try:\n            checkpoint = torch.load(path, map_location=self.pipeline.device)\n            \n            # Load LoRA weights\n            for name, lora_layer in self.pipeline.lora_layers.items():\n                if name in checkpoint['lora_state_dict']:\n                    state_dict = checkpoint['lora_state_dict'][name]\n                    lora_layer.lora_A.data = state_dict['lora_A']\n                    lora_layer.lora_B.data = state_dict['lora_B']\n                    lora_layer.scaling = state_dict['scaling'].item()\n            \n            # Load optimizer and scheduler\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            \n            # Load training history\n            self.train_losses = checkpoint.get('train_losses', [])\n            self.val_losses = checkpoint.get('val_losses', [])\n            \n            logger.info(f\"Model loaded from {path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading model: {e}\")\n            raise\n\ndef create_dataloaders(\n    original_dir: str, \n    styled_dir: str, \n    style_name: str, \n    config: TrainingConfig\n) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Create train and validation dataloaders\"\"\"\n    \n    # Create full dataset\n    full_dataset = StyleTransferDataset(\n        original_dir=original_dir,\n        styled_dir=styled_dir,\n        style_name=style_name,\n        image_size=config.image_size,\n        augment=True,\n        validate_pairs=True,\n    )\n    \n    # Split dataset\n    train_size = int((1 - config.validation_split) * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    \n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size]\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=True,\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=False,\n    )\n    \n    return train_loader, val_loader\n\ndef enhanced_test_inference(pipeline, style_name, original_dir, output_dir, num_test_images=3):\n    \"\"\"Enhanced test inference with multiple images and better handling\"\"\"\n    logger.info(\"Testing inference...\")\n    \n    # Find test images\n    test_files = list(Path(original_dir).glob('*.[jp][pn]g'))\n    test_files.extend(list(Path(original_dir).glob('*.[JP][PN]G')))\n    \n    if not test_files:\n        logger.warning(\"No test images found in original directory\")\n        return\n    \n    # Test with multiple images (or just the first few)\n    num_tests = min(num_test_images, len(test_files))\n    \n    for i in range(num_tests):\n        test_image_path = test_files[i]  # Fixed: use index instead of entire list\n        logger.info(f\"Testing with image {i+1}/{num_tests}: {test_image_path.name}\")\n        \n        try:\n            styled_result = pipeline.stylize_image(\n                image=str(test_image_path),\n                prompt=f\"studio ghibli style, anime artwork, soft watercolor painting, detailed, masterpiece\",\n                strength=0.8,\n                guidance_scale=12.0,\n                num_inference_steps=75\n            )\n            \n            # Save with descriptive filename\n            test_output_path = f\"{output_dir}/test_result_{i+1}_{test_image_path.stem}.png\"\n            styled_result.save(test_output_path)\n            logger.info(f\"Test result {i+1} saved to {test_output_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error processing test image {test_image_path.name}: {e}\")\n            continue\n    \n    logger.info(f\"Test inference completed. Results saved in {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:23:35.234459Z","iopub.execute_input":"2025-06-30T14:23:35.234677Z","iopub.status.idle":"2025-06-30T14:23:35.258638Z","shell.execute_reply.started":"2025-06-30T14:23:35.234662Z","shell.execute_reply":"2025-06-30T14:23:35.258147Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main():\n    \"\"\"Main training function with all improvements\"\"\"\n    # Enhanced configuration\n    config = TrainingConfig(\n        batch_size=2,\n        effective_batch_size=8,\n        num_epochs=25,  # Increased for better training\n        learning_rate=5e-5,  # Slightly higher learning rate\n        save_every=10,\n        lora_rank=128,  # Higher rank for more capacity\n        lora_alpha=128,\n        use_enhanced_loss=True,\n        perceptual_loss_weight=0.1\n    )\n    \n    # Setup directories - CUSTOMIZE THESE PATHS\n    original_dir = \"/kaggle/input/stylized-images/stylized_images/original_images1\"  # Path to original images\n    styled_dir = \"/kaggle/input/stylized-images/stylized_images/gibli_stylized_images1\"      # Path to styled images\n    style_name = \"ghibli\"                          \n    output_dir = \"outputs\"\n    \n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Validate directories\n    if not os.path.exists(original_dir) or not os.path.exists(styled_dir):\n        logger.error(\"Please update the directory paths in the main() function!\")\n        logger.error(f\"Looking for:\")\n        logger.error(f\"  Original images: {original_dir}\")\n        logger.error(f\"  Styled images: {styled_dir}\")\n        return\n    \n    # Setup device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n    logger.info(f\"Using device: {device} with dtype: {torch_dtype}\")\n    \n    try:\n        # Initialize pipeline and trainer\n        pipeline = DiffusionStyleTransferPipeline(\n            config=config,\n            device=device,\n            torch_dtype=torch_dtype\n        )\n        \n        trainer = StyleTransferTrainer(pipeline, config)\n        \n        # Create dataloaders\n        train_loader, val_loader = create_dataloaders(\n            original_dir, styled_dir, style_name, config\n        )\n        logger.info(f\"Created dataloaders: {len(train_loader)} train, {len(val_loader)} val batches\")\n        \n        # Training loop\n        logger.info(\"Starting training...\")\n        best_val_loss = float('inf')\n        \n        for epoch in range(config.num_epochs):\n            # Train epoch\n            avg_train_loss = trainer.train_epoch(train_loader, epoch)\n            \n            # Validate\n            avg_val_loss = trainer.validate(val_loader)\n            \n            logger.info(\n                f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, \"\n                f\"Val Loss = {avg_val_loss:.4f}, \"\n                f\"LR = {trainer.optimizer.param_groups[0]['lr']:.2e}\"\n            )\n            \n            # Save best model\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_path = f\"{output_dir}/best_model.pt\"\n                trainer.save_model(best_path, epoch)\n            \n            # Save periodic checkpoints\n            if (epoch + 1) % config.save_every == 0:\n                checkpoint_path = f\"{output_dir}/checkpoint_epoch_{epoch+1}.pt\"\n                trainer.save_model(checkpoint_path, epoch)\n        \n        # Save final model\n        final_path = f\"{output_dir}/final_model.pt\"\n        trainer.save_model(final_path, config.num_epochs)\n        \n        logger.info(\"Training completed successfully!\")\n        \n        # Enhanced test inference\n        enhanced_test_inference(pipeline, style_name, original_dir, output_dir, num_test_images=3)\n        \n    except Exception as e:\n        logger.error(f\"Training failed with error: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:28:21.074851Z","iopub.execute_input":"2025-06-30T14:28:21.075163Z","iopub.status.idle":"2025-06-30T14:31:24.516821Z","shell.execute_reply.started":"2025-06-30T14:28:21.075134Z","shell.execute_reply":"2025-06-30T14:31:24.516069Z"}},"outputs":[{"name":"stderr","text":"Epoch 0: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it, Total=0.0622, Diffusion=0.0622, Perceptual=0.0000, LR=5.00e-05]\nEpoch 1: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it, Total=0.0195, Diffusion=0.0195, Perceptual=0.0000, LR=4.98e-05]\nEpoch 2: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it, Total=0.0192, Diffusion=0.0192, Perceptual=0.0000, LR=4.93e-05]\nEpoch 3: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it, Total=0.0680, Diffusion=0.0680, Perceptual=0.0000, LR=4.84e-05]\nEpoch 4: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it, Total=0.1955, Diffusion=0.1955, Perceptual=0.0000, LR=4.72e-05]\nEpoch 5: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it, Total=0.0808, Diffusion=0.0808, Perceptual=0.0000, LR=4.57e-05]\nEpoch 6: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it, Total=0.1037, Diffusion=0.1037, Perceptual=0.0000, LR=4.39e-05]\nEpoch 7: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it, Total=0.0880, Diffusion=0.0880, Perceptual=0.0000, LR=4.18e-05]\nEpoch 8: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it, Total=0.0901, Diffusion=0.0901, Perceptual=0.0000, LR=3.96e-05]\nEpoch 9: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it, Total=0.2026, Diffusion=0.2026, Perceptual=0.0000, LR=3.71e-05]\nEpoch 10: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it, Total=0.0747, Diffusion=0.0747, Perceptual=0.0000, LR=3.45e-05]\nEpoch 11: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it, Total=0.4621, Diffusion=0.4621, Perceptual=0.0000, LR=3.17e-05]\nEpoch 12: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it, Total=0.0292, Diffusion=0.0292, Perceptual=0.0000, LR=2.89e-05]\nEpoch 13: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it, Total=0.0079, Diffusion=0.0079, Perceptual=0.0000, LR=2.61e-05]\nEpoch 14: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it, Total=0.0338, Diffusion=0.0338, Perceptual=0.0000, LR=2.33e-05]\nEpoch 15: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it, Total=0.0476, Diffusion=0.0476, Perceptual=0.0000, LR=2.05e-05]\nEpoch 16: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it, Total=0.0748, Diffusion=0.0748, Perceptual=0.0000, LR=1.79e-05]\nEpoch 17: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it, Total=0.2189, Diffusion=0.2189, Perceptual=0.0000, LR=1.54e-05]\nEpoch 18: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it, Total=0.1186, Diffusion=0.1186, Perceptual=0.0000, LR=1.32e-05]\nEpoch 19: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it, Total=0.1356, Diffusion=0.1356, Perceptual=0.0000, LR=1.11e-05]\nEpoch 20: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it, Total=0.1165, Diffusion=0.1165, Perceptual=0.0000, LR=9.30e-06]\nEpoch 21: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, Total=0.3146, Diffusion=0.3146, Perceptual=0.0000, LR=7.78e-06]\nEpoch 22: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it, Total=0.0702, Diffusion=0.0702, Perceptual=0.0000, LR=6.58e-06]\nEpoch 23: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it, Total=0.0837, Diffusion=0.0837, Perceptual=0.0000, LR=5.71e-06]\nEpoch 24: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it, Total=0.1880, Diffusion=0.1880, Perceptual=0.0000, LR=5.18e-06]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2b628f40264838a2ff6542a6362a94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d592e5aa5f5b4f1e8300eb575f1f8a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1310b86b491b42e59cb2d6ea35c45d94"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}